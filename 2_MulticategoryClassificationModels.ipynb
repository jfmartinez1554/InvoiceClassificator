{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43358a47",
   "metadata": {},
   "source": [
    "# Comparativa de modelos de clasificación de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192e8b5",
   "metadata": {},
   "source": [
    "Iniciamos adecuando el ambiente de ejcución, instalando e importando los paquetes necesarios para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8aef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install xgboost\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeff7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ed843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_image_data_format(\"channels_last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a011806",
   "metadata": {},
   "source": [
    "Llamamos del notebook que construye el dataset vectorizado mediante TF-IDF y genera nuestros conjuntos de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30261aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1125)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1125)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-3fec7da56bed>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_facturas['Categoria'] = df_facturas['issuer'].map(diccionario)\n",
      "<ipython-input-4-3fec7da56bed>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no_facturas['Categoria'] = \"No factura\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sfarma are drogas fragua nit sparma droguerias cipriano calle san cipriano tel aut numeracion fac regimen comun tarifa tarifa regimen regimen comun obs regimen grandes contribuyentes fecha emision feoha vigencia prefilo consecutivos factura venta mostrador fecha hora caja ciudad bogota vendedor daniela alejandra carreno castellano nombre cliente primer apellido generico segundo apellido kkkkkkk cedula nit direccion calle telefono producto cant pvp dtd imp valor buprofend metocarbamol tabletas pres und levotironina tes removedor lander vitaminae mad schickquattro uds pres fraccion und paleta tosh pasion palet apolet surtica betametasona cpema nitazoxanda tbs crema corega sulfato magnesia drogam melterios pastillas masticables sbs pres sobri und noraver grpabebidanoche sbs vich vaporub ups pres fraccion unid flumuc age mes sbs pres fraccion und pax dia naranja ses pres fraccion xiund tapaboca termosellado biokemical soul pres fraccion lund paletaje surtida labinpina tabletas pres und bood trimebutina tabletas alsucral tabletas totales venta total imp descuento impuestos total total pagar recibido master card efectivo total recibido ciento ochenta nueve mil pesos discriminacion impuestos tipoimp base imp imp venta exento iva master card codigo aprobac condiciones cambio numero tarjeta garantia garantias debe presentar factura venta productos promocion deneral cambio hacen vez devoluciones farmaceutico dinero medicamento eficacia procedido dispensarte podemos garantizar medicamento conservado condiciones optimas garantizar seguridad podernos garantizar sido pilado expuesto unas condiciones inadecuadas conservación temperaturas excesivamente elevades sometidos humedad medicamentos exposición luz solar aceptal devolución\n",
      "1136\n",
      "X_train shape: (1136, 17756)\n",
      "X_test shape: (487, 17756)\n",
      "y_train shape: (1136,)\n",
      "y_test shape: (487,)\n"
     ]
    }
   ],
   "source": [
    "%run 1_MulticategoricalTFIDF.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913caf72",
   "metadata": {},
   "source": [
    "Dado que la variable de respuesta cuenta con diferentes categorías tipo \"string\", a continuación se codifica la variable a una matriz numérica binaria para poder entrenar las redes neuronales más adelante. Este proceso no es necesario para los demas modelos (SVM, Random Forest, XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f50c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_d = pd.get_dummies(y_train)\n",
    "y_test_d = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e622c90",
   "metadata": {},
   "source": [
    "## Definición de modelos de clasificación de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6d9e5",
   "metadata": {},
   "source": [
    "Para este proyecto se utilizarán los siguientes modelos, cuyos parámetros fueron ajustados y seleccionados mediante un grid search con validación cruzada:\n",
    "\n",
    "* **Linear Support Vector Classifier:**\n",
    "  * Parámetro de regularización: 10\n",
    "  * Función de pérdida: squared hinge\n",
    "  * Máximo número de iteraciones: 1.000\n",
    "  * Algoritmo para resolver el problema de optimización: Dual\n",
    "  * Estrategia multiclase: ovr, es decir que entrena n-clases uno contra el resto\n",
    "  * Norma utilizada en la penalización: 'l2', es decir el estándar utilizado en SVC\n",
    "<br>\n",
    "<br>\n",
    "* **XGBoost Classifier:**\n",
    "  * Tipo de refuerzo \"Booster\": gbtree, utiliza modelos basados en árboles de decisión\n",
    "  * Tasa de aprendizaje: 0.3\n",
    "  * Máxima profundidad de los árboles de decisión: 6\n",
    "<br>\n",
    "<br>\n",
    "* **Random Forest Classifier:**\n",
    "  * Función para medir la calidad de la división en los arboles: Entropía\n",
    "  * Máxima profundidad de los árboles de decisión: 20\n",
    "  * Máximo número de características a tener en cuenta al buscar la mejor división en los árboles: sqrt, es decir que max_features = sqrt(n_features)\n",
    "  * Máximo de nodos hoja: 1.000\n",
    "  * Número mínimo de muestras requeridas para estar en un nodo de hoja: 10\n",
    "  * Número de árboles: 10.000\n",
    "<br>\n",
    "<br>\n",
    "* **Perceptrón multicapa:**\n",
    "  * Estructura:\n",
    "      * Red neuronal densamente conectada\n",
    "      * Una capa oculta con 256 neuronas y función de actuvación \"relu\"\n",
    "      * Capa de salida con 5 neuronas y función de activación \"softmax\"\n",
    "  * Optimización:\n",
    "      * Función de pérdida: Crossentropía binaria\n",
    "      * Optimizador: ADAM\n",
    "      * Épocas: 10\n",
    "      * Tamaño de los lotes: 64\n",
    "<br>\n",
    "<br>\n",
    "* **Red Neuronal LSTM:**\n",
    "  * Estructura:\n",
    "      * Red neuronal recurrente LSTM\n",
    "      * Capa de embedding con dimensión de 100, máximo de 50.000 palabras, y secuencia máxima de 250 caracteres\n",
    "      * Capa de Long short-term memory con 256 unidades ocultas, un dropout y dropout del estado recurrente del 20%\n",
    "      * Una capa oculta densamente conectada con 128 neuronas y función de actuvación \"relu\"\n",
    "      * Capa de salida densamente conectada con 5 neuronas y función de activación \"softmax\"\n",
    "  * Optimización:\n",
    "      * Función de pérdida: Crossentropía binaria\n",
    "      * Optimizador: ADAM\n",
    "      * Épocas: 50\n",
    "      * Tamaño de los lotes: 128\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b91248fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_clf(X_train,X_test,y_train,y_test):\n",
    "    clf = LinearSVC(C=10, loss='squared_hinge', max_iter= 1000,\n",
    "                    dual = True, multi_class= 'ovr',\n",
    "                    penalty= 'l2', random_state= 42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    accuracy = accuracy_score (y_test, preds)\n",
    "    return accuracy,preds\n",
    "\n",
    "\n",
    "def xgb_clf(X_train,X_test,y_train,y_test):\n",
    "    clf = xgb.XGBClassifier(booster = 'gbtree', \n",
    "                            eta = 0.3, max_depth = 6)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    accuracy = accuracy_score (y_test, preds)\n",
    "    return accuracy,preds\n",
    "\n",
    "\n",
    "def randf_clf(X_train,X_test,y_train,y_test):\n",
    "    clf = RandomForestClassifier(\n",
    "        criterion = 'entropy', max_depth = 20,\n",
    "        max_features = 'sqrt', max_leaf_nodes = 1000,\n",
    "        min_samples_leaf = 10, n_estimators = 10000)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    accuracy = accuracy_score (y_test, preds)\n",
    "    return accuracy,preds\n",
    "\n",
    "\n",
    "def perceptron(X_train, X_test, y_train, y_test):\n",
    "    input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(256, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=50,\n",
    "                        verbose=False,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        batch_size=128)\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "    preds = model.predict(X_test)\n",
    "    preds = (preds > 0.5).astype(\"int32\")\n",
    "    return accuracy,preds\n",
    "\n",
    "\n",
    "\n",
    "def lstm(dataframe):\n",
    "    \n",
    "    MAX_NB_WORDS = 50000\n",
    "    MAX_SEQUENCE_LENGTH = 250\n",
    "    EMBEDDING_DIM = 100\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "    tokenizer.fit_on_texts(dataframe['text'].values)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    X = tokenizer.texts_to_sequences(dataframe['text'].values)\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    Y = pd.get_dummies(dataframe['Categoria']).values\n",
    "    \n",
    "    X_train_lstm, X_test_lstm, Y_train_lstm, Y_test_lstm = train_test_split(X,Y, test_size = 0.30, stratify = dataframe.Categoria, random_state = 42)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "    model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "\n",
    "    history = model.fit(X_train_lstm, Y_train_lstm, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "    accuracy = model.evaluate(X_test_lstm,Y_test_lstm)\n",
    "    preds = model.predict(X_test_lstm)\n",
    "    preds = (preds > 0.5).astype(\"int32\")\n",
    "    \n",
    "    return accuracy,preds,Y_test_lstm\n",
    "\n",
    "\n",
    "\n",
    "def acc_promedio(n, modelo):\n",
    "    suma_acuracia = 0\n",
    "    for i in range(n):\n",
    "        suma_acuracia += modelo[0]\n",
    "    return  suma_acuracia/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad245951",
   "metadata": {},
   "source": [
    "## Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4375a7fa",
   "metadata": {},
   "source": [
    "A continuación entrenaremos los modelos previamente definidos, y calcularemos el tiempo de entrenamiento con el fin de incluir este indicador como variable de desempeño a comparar para la selección del mejor algoritmo de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2863d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9630390143737166\n",
      "CPU times: user 263 ms, sys: 24.5 ms, total: 287 ms\n",
      "Wall time: 251 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "acc_svc, pred_svc = svc_clf(X_train,X_test,y_train,y_test)\n",
    "print(acc_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "879bfb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:27:01] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9589322381930184\n",
      "CPU times: user 16min 40s, sys: 37.5 s, total: 17min 18s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "acc_xgb, pred_xgb = xgb_clf(X_train,X_test,y_train,y_test)\n",
    "print(acc_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfb7d5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8562628336755647\n",
      "CPU times: user 1min 53s, sys: 599 ms, total: 1min 54s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "acc_rfc, pred_rfc = randf_clf(X_train,X_test,y_train,y_test)\n",
    "print(acc_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f5273f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9650924205780029\n",
      "CPU times: user 1min 32s, sys: 37 s, total: 2min 9s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "acc_perc, pred_perc = perceptron(X_train,X_test,y_train_d,y_test_d)\n",
    "print(acc_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dcfcc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 17s 972ms/step - loss: 1.4727 - accuracy: 0.3855 - val_loss: 1.3758 - val_accuracy: 0.5263\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 15s 970ms/step - loss: 1.1789 - accuracy: 0.6047 - val_loss: 1.0808 - val_accuracy: 0.4474\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 24s 2s/step - loss: 0.6419 - accuracy: 0.7769 - val_loss: 0.5159 - val_accuracy: 0.8070\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 66s 4s/step - loss: 0.2915 - accuracy: 0.9070 - val_loss: 0.3925 - val_accuracy: 0.8772\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 26s 2s/step - loss: 0.1569 - accuracy: 0.9432 - val_loss: 0.3110 - val_accuracy: 0.9123\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 27s 2s/step - loss: 0.0903 - accuracy: 0.9560 - val_loss: 0.3107 - val_accuracy: 0.8860\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 40s 3s/step - loss: 0.0521 - accuracy: 0.9795 - val_loss: 0.2601 - val_accuracy: 0.9123\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 37s 2s/step - loss: 0.0299 - accuracy: 0.9951 - val_loss: 0.3948 - val_accuracy: 0.8509\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.0309 - accuracy: 0.9912 - val_loss: 0.4879 - val_accuracy: 0.8421\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 46s 3s/step - loss: 0.0424 - accuracy: 0.9902 - val_loss: 0.3007 - val_accuracy: 0.9298\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.3998 - accuracy: 0.9014\n",
      "[0.3998074531555176, 0.9014373421669006]\n",
      "CPU times: user 21min 24s, sys: 26min 10s, total: 47min 35s\n",
      "Wall time: 5min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "acc_lstm, pred_lstm, Y_test_lstm = lstm(dataframe)\n",
    "print(acc_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa923a7",
   "metadata": {},
   "source": [
    "Como se puede observar, el modelo con menor tiempo de entrenamiento es el Linear Support Vector Classifier, seguido por el percptrón multicapa. Por el contrario, la red neuronal LSTM es la que cuenta con un mayor tiempo de entrenamiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac18d2",
   "metadata": {},
   "source": [
    "## Comparativo de desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb3f006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informe de desempeño SVC:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Alimentación       0.95      0.98      0.97       170\n",
      "Grande superficie       0.95      0.97      0.96        99\n",
      "             Moda       0.93      0.78      0.85        18\n",
      "       No factura       0.99      0.98      0.99       158\n",
      "Salud y Bienestar       0.93      0.90      0.92        42\n",
      "\n",
      "         accuracy                           0.96       487\n",
      "        macro avg       0.95      0.92      0.94       487\n",
      "     weighted avg       0.96      0.96      0.96       487\n",
      "\n",
      "Informe de desempeño XGB:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Alimentación       0.93      0.99      0.96       170\n",
      "Grande superficie       0.94      0.97      0.96        99\n",
      "             Moda       0.93      0.72      0.81        18\n",
      "       No factura       1.00      0.98      0.99       158\n",
      "Salud y Bienestar       0.97      0.81      0.88        42\n",
      "\n",
      "         accuracy                           0.96       487\n",
      "        macro avg       0.95      0.90      0.92       487\n",
      "     weighted avg       0.96      0.96      0.96       487\n",
      "\n",
      "Informe de desempeño Random Forest:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Alimentación       0.75      0.98      0.85       170\n",
      "Grande superficie       0.93      0.89      0.91        99\n",
      "             Moda       0.00      0.00      0.00        18\n",
      "       No factura       0.94      0.97      0.96       158\n",
      "Salud y Bienestar       1.00      0.24      0.38        42\n",
      "\n",
      "         accuracy                           0.86       487\n",
      "        macro avg       0.73      0.61      0.62       487\n",
      "     weighted avg       0.84      0.86      0.82       487\n",
      "\n",
      "Informe de desempeño Perceptrón Multicapa:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       170\n",
      "           1       0.96      0.97      0.96        99\n",
      "           2       0.93      0.78      0.85        18\n",
      "           3       0.97      0.97      0.97       158\n",
      "           4       0.97      0.88      0.93        42\n",
      "\n",
      "   micro avg       0.97      0.96      0.96       487\n",
      "   macro avg       0.96      0.92      0.94       487\n",
      "weighted avg       0.97      0.96      0.96       487\n",
      " samples avg       0.96      0.96      0.96       487\n",
      "\n",
      "Informe de desempeño LSTM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90       170\n",
      "           1       0.91      0.94      0.93        99\n",
      "           2       0.69      0.50      0.58        18\n",
      "           3       0.99      0.91      0.95       158\n",
      "           4       0.87      0.81      0.84        42\n",
      "\n",
      "   micro avg       0.92      0.90      0.91       487\n",
      "   macro avg       0.87      0.82      0.84       487\n",
      "weighted avg       0.92      0.90      0.91       487\n",
      " samples avg       0.90      0.90      0.90       487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "print(\"Informe de desempeño SVC:\")\n",
    "print(classification_report(y_test, pred_svc))\n",
    "\n",
    "print(\"Informe de desempeño XGB:\")\n",
    "print(classification_report(y_test, pred_xgb))\n",
    "\n",
    "print(\"Informe de desempeño Random Forest:\")\n",
    "print(classification_report(y_test, pred_rfc))\n",
    "\n",
    "print(\"Informe de desempeño Perceptrón Multicapa:\")\n",
    "print(classification_report(y_test_d, pred_perc))\n",
    "\n",
    "print(\"Informe de desempeño LSTM:\")\n",
    "print(classification_report(Y_test_lstm, pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c80e8",
   "metadata": {},
   "source": [
    "Luego de analizar la evaluación de desmpeño de cada modelo, encontramos que los más eficientes y al mismo tiempo, los que mejor se ajustan a los datos, sin caer aparentemente en sobreajuste son el Linear Support Vector Classifier y el Perceptrón Multicapa.\n",
    "\n",
    "En efecto, tanto el LinearSVC como el perceptrón multicapa muestran unas métricas F1-Score por encima del 90% para todas las categorías, menos para \"moda\", para la cual registran un desempeño del 85%, a pesar de contar con un bajo número de registros para entrenar el modelo. \n",
    "\n",
    "En cuanto al XGBoost, se observan métricas bastante similares, por encima del 90% para todas las categorías, sin embargo, para \"salud y bienestar\" y \"moda\" el desempeño se reduce para ubicarse en 88% y 81%. respectivamente.\n",
    "\n",
    "Para el caso de la red LSTM se observa unas métricas F1-Score por encima del 80% para todas las categorías, menos para \"salud y bienestar\" y \"moda\", para los cuales se registran un desempeño de 79% y 50%, respectivamente. Lo anterior se puede explicar por el bajo número de registros para entrenar el modelo en dichas categorías, lo cual se ve profundizado teniendo en cuenta que este tipo de redes requieren de un gran número de registros para entrenarse adecuadamente.\n",
    "\n",
    "Finalmente, el Random Forest evidencia que el algortimo no logran ajustarse del todo a los datos, en especial para la categoría \"moda\", la cual no logra ser identificada en ninguna de las observaciones de evaluación. Como resultado se observa el Accuracy global más bajo entre todos los modelos.\n",
    "\n",
    "De esta manera, se selecciona en primer lugar al Linear Support Vector Classifier dado que es el que cuenta con mejores métricas y al mismo tiempo un menor tiempo de entrenamiento. En segundo lugar se selecciona al perceptrón multicapa, dado que cuenta con métricas igual de satisfactorias al LinearSVC, aunque con un tiempo de entreno mayor. Finalmente se descartan los demás modelos dado que muestran peores métricas, al mismo tiempo en que requieren un mayor tiempo de entrenamiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef84f234",
   "metadata": {},
   "source": [
    "### Guardar los modelos entrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f6a07",
   "metadata": {},
   "source": [
    "Ahora que hemos seleccionado los modelos a utilizar para poner a prueba el algoritmo final de clasificación, guardaremos los modelos entrenados con el set de datos de entrenamiento, para aplicarlos dentro del algoritmo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd6e057a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVM.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC(C=10, loss='squared_hinge', max_iter= 1000,\n",
    "                dual = True, multi_class= 'ovr',\n",
    "                penalty= 'l2', random_state= 42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "filename = \"SVM.joblib\"\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "160b1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(256, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history= model.fit(X_train, y_train_d,\n",
    "                   epochs=50,\n",
    "                   verbose=False,\n",
    "                   batch_size=128)\n",
    "\n",
    "model.save(\"Perceptron.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea4972",
   "metadata": {},
   "source": [
    "### Ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c238f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a025601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensamble (X): \n",
    "    \n",
    "    modelo_svm = joblib.load(\"SVM.joblib\")\n",
    "    pred_svm = modelo_svm.predict(X)\n",
    "\n",
    "    modelo_perceptron = keras.models.load_model(\"Perceptron.h5\")\n",
    "    pred_perceptron = modelo_perceptron.predict(X)\n",
    "    pred_perceptron = (pred_perceptron >= 0.5).astype(\"int32\")\n",
    "    \n",
    "    pred_perc = []\n",
    "    lst = ['Alimentación', 'Grande superficie', 'Moda', 'No factura', 'Salud y Bienestar']  \n",
    "    for i in pred_perceptron:\n",
    "        maxindex = np.argmax(i)\n",
    "        pred = lst[maxindex]\n",
    "        pred_perc.append(pred)\n",
    "    \n",
    "    pred=[]\n",
    "    for i in range(len(pred_svm)):\n",
    "        if pred_svm[i] == 'No factura':\n",
    "            pred.append(pred_svm[i])\n",
    "        else:\n",
    "            pred.append(pred_perc[i])\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa4e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ensamble = ensamble(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3f86f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_svm = joblib.load(\"SVM.joblib\")\n",
    "pred_svm = modelo_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ddeda23",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_perceptron = keras.models.load_model(\"Perceptron.h5\")\n",
    "pred_perceptron = modelo_perceptron.predict(X_test)\n",
    "pred_perceptron = (pred_perceptron >= 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20991f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informe de desempeño Ensamble:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Alimentación       0.95      0.98      0.97       170\n",
      "Grande superficie       0.95      0.97      0.96        99\n",
      "             Moda       0.93      0.78      0.85        18\n",
      "       No factura       0.97      0.98      0.98       158\n",
      "Salud y Bienestar       0.97      0.86      0.91        42\n",
      "\n",
      "         accuracy                           0.96       487\n",
      "        macro avg       0.96      0.91      0.93       487\n",
      "     weighted avg       0.96      0.96      0.96       487\n",
      "\n",
      "Informe de desempeño SVC:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Alimentación       0.95      0.98      0.97       170\n",
      "Grande superficie       0.95      0.97      0.96        99\n",
      "             Moda       0.93      0.78      0.85        18\n",
      "       No factura       0.99      0.98      0.99       158\n",
      "Salud y Bienestar       0.93      0.90      0.92        42\n",
      "\n",
      "         accuracy                           0.96       487\n",
      "        macro avg       0.95      0.92      0.94       487\n",
      "     weighted avg       0.96      0.96      0.96       487\n",
      "\n",
      "Informe de desempeño Perceptrón:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       170\n",
      "           1       0.95      0.97      0.96        99\n",
      "           2       0.93      0.78      0.85        18\n",
      "           3       0.98      0.97      0.97       158\n",
      "           4       0.97      0.88      0.93        42\n",
      "\n",
      "   micro avg       0.97      0.96      0.96       487\n",
      "   macro avg       0.96      0.92      0.94       487\n",
      "weighted avg       0.97      0.96      0.96       487\n",
      " samples avg       0.96      0.96      0.96       487\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Informe de desempeño Ensamble:\")\n",
    "print(classification_report(y_test, pred_ensamble))\n",
    "\n",
    "print(\"Informe de desempeño SVC:\")\n",
    "print(classification_report(y_test, pred_svm))\n",
    "\n",
    "print(\"Informe de desempeño Perceptrón:\")\n",
    "print(classification_report(y_test_d, pred_perceptron))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a977bdd",
   "metadata": {},
   "source": [
    "El ensamble no sirve porque incluso cuando ha pasado el filtro del SVC identificando que es una factura, le da el chance al perceptrón de que se equivoque clasificandola como una no factura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf8b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
