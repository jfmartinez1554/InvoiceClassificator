{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a86cec7",
   "metadata": {},
   "source": [
    "# Pre-Procesamiento para NLP mediante TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae149b",
   "metadata": {},
   "source": [
    "Este notebook será la plantilla que se utilizará para realizar el pre - procesamiento de los datos y darle uso a TF-IDF. con el fin de utilizar la base final como base de los siguientes modelos en el desarrollo del ejercicio:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bc11d0",
   "metadata": {},
   "source": [
    "## Obtención de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31a8f6",
   "metadata": {},
   "source": [
    "Comenzaremos con traer los datos necesarios e importar los paquetes requeridos para el desarrollo del ejercicio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e63fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Constants, imports and utilitary functions\n",
    "%run 0_Utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ff13b",
   "metadata": {},
   "source": [
    "importamos los paquetes que utilizaremos para el procesamiento de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63af4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611239e7-006e-4e2e-9f83-5e8668d4d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1125)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1125)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3d6f3-cfee-49d1-af67-492e0fddc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta celda solo debe ser usada en caso de que la celda anterior falle y no sea posible descargar los paquetes de NLTK.\n",
    "# https://stackoverflow.com/questions/38916452/nltk-download-ssl-certificate-verify-failed\n",
    "\n",
    "#import nltk\n",
    "#import ssl\n",
    "\n",
    "#try:\n",
    "#    _create_unverified_https_context = ssl._create_unverified_context\n",
    "#except AttributeError:\n",
    "#    pass\n",
    "#else:\n",
    "#    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01dda1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-3fec7da56bed>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_facturas['Categoria'] = df_facturas['issuer'].map(diccionario)\n",
      "<ipython-input-2-3fec7da56bed>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no_facturas['Categoria'] = \"No factura\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1783, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = getLatestDataset()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72214c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1753, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['text'].notna()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98fc12d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1753, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['Categoria'].notna()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1c82f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categoria\n",
       "Alimentación             567\n",
       "No factura               527\n",
       "Grande superficie        330\n",
       "Salud y Bienestar        139\n",
       "Moda                      60\n",
       "Transporte                33\n",
       "Educación                 29\n",
       "Hogar                     29\n",
       "Entretenimiento           13\n",
       "Mascotas                  13\n",
       "Compromisos Bancarios      7\n",
       "Tecnología                 4\n",
       "Impuestos                  2\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Categoria')['text'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "737af028",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=df[(df['Categoria'] == 'Alimentación')|\n",
    "          (df['Categoria'] == 'No factura')|\n",
    "          (df['Categoria'] == 'Grande superficie')|\n",
    "          (df['Categoria'] == 'Salud y Bienestar')|\n",
    "          (df['Categoria'] == 'Moda')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97824143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categoria\n",
       "Alimentación         567\n",
       "No factura           527\n",
       "Grande superficie    330\n",
       "Salud y Bienestar    139\n",
       "Moda                  60\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.groupby('Categoria')['text'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b3d90",
   "metadata": {},
   "source": [
    "# Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da3038",
   "metadata": {},
   "source": [
    "Primero, realizaremos la división del dataset entre las bases de Test y las bases de Train, con el fin de evitar Data Leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e96be4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos los paquetes requeridos\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e16418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#realizamos la separación, seleccionando que el 20% de los datos sean parte del set de test:\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataframe.text, dataframe.Categoria, test_size = 0.30, stratify = dataframe.Categoria, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02442a7b",
   "metadata": {},
   "source": [
    "Para trabajar con texto tenemos que realizar una serie de pasos antes de poder entrenar un modelo. Es decir, buscar la forma de convertir el texto a una representación numérica que pueda ser interpretable por los algoritmos de clasificación. \n",
    "Para ello vamos a realizar una serie de pasos.\n",
    "- Tokenizar el texto\n",
    "- Convertir a vectores de términos/documentos\n",
    "- Aplicar tfidf\n",
    "\n",
    "Es importante destacar que el `fit` debe hacerse sobre el conjunto de `train` y no sobre el total, ya que `tfidf` tiene en cuenta la frecuencia de aparición de las palabras respecto al total del conjunto. Una vez que usamos el `fit` con el conjunto de entrenamiento podemos aplicar la transformación al conjunto de `test`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec9ee5f",
   "metadata": {},
   "source": [
    "## Pre- Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "148722e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eea1c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('spanish')]\n",
    "    #stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    #lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b45f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [preprocess(item) for item in X_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22dfedb",
   "metadata": {},
   "source": [
    "Sanity check - debemos obtener un ejemplo de un texto \"limpio\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b08f2799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sfarma are drogas fragua nit sparma droguerias cipriano calle san cipriano tel aut numeracion fac regimen comun tarifa tarifa regimen regimen comun obs regimen grandes contribuyentes fecha emision feoha vigencia prefilo consecutivos factura venta mostrador fecha hora caja ciudad bogota vendedor daniela alejandra carreno castellano nombre cliente primer apellido generico segundo apellido kkkkkkk cedula nit direccion calle telefono producto cant pvp dtd imp valor buprofend metocarbamol tabletas pres und levotironina tes removedor lander vitaminae mad schickquattro uds pres fraccion und paleta tosh pasion palet apolet surtica betametasona cpema nitazoxanda tbs crema corega sulfato magnesia drogam melterios pastillas masticables sbs pres sobri und noraver grpabebidanoche sbs vich vaporub ups pres fraccion unid flumuc age mes sbs pres fraccion und pax dia naranja ses pres fraccion xiund tapaboca termosellado biokemical soul pres fraccion lund paletaje surtida labinpina tabletas pres und bood trimebutina tabletas alsucral tabletas totales venta total imp descuento impuestos total total pagar recibido master card efectivo total recibido ciento ochenta nueve mil pesos discriminacion impuestos tipoimp base imp imp venta exento iva master card codigo aprobac condiciones cambio numero tarjeta garantia garantias debe presentar factura venta productos promocion deneral cambio hacen vez devoluciones farmaceutico dinero medicamento eficacia procedido dispensarte podemos garantizar medicamento conservado condiciones optimas garantizar seguridad podernos garantizar sido pilado expuesto unas condiciones inadecuadas conservación temperaturas excesivamente elevades sometidos humedad medicamentos exposición luz solar aceptal devolución\n",
      "1136\n"
     ]
    }
   ],
   "source": [
    "print(X_train[15])\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3471e0",
   "metadata": {},
   "source": [
    "## Aplicar TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abde7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# los pasos necesarios para vectorizar los conjuntos de entrenamiento y testeo en este celda\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "serie = pd.Series( X_train )\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_tfidf_matrix = vectorizer.fit_transform(serie)\n",
    "test_tfidf_matrix = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ba423-5ed0-4967-bea4-3483e893770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "filename = \"TFIDF-vectorizer.joblib\"\n",
    "joblib.dump(vectorizer, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82a0e78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1136, 17756)\n",
      "X_test shape: (487, 17756)\n",
      "y_train shape: (1136,)\n",
      "y_test shape: (487,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_tfidf_matrix.todense()\n",
    "X_test = test_tfidf_matrix.todense()\n",
    "\n",
    "print( \"X_train shape: \" + str(X_train.shape))\n",
    "print( \"X_test shape: \" + str(X_test.shape ))\n",
    "print( \"y_train shape: \" + str(y_train.shape )) \n",
    "print( \"y_test shape: \" + str(y_test.shape ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f786c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1182           No factura\n",
       "246          Alimentación\n",
       "1092    Grande superficie\n",
       "107     Grande superficie\n",
       "1065         Alimentación\n",
       "              ...        \n",
       "377     Grande superficie\n",
       "1170         Alimentación\n",
       "1723         Alimentación\n",
       "539     Grande superficie\n",
       "986          Alimentación\n",
       "Name: Categoria, Length: 1136, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1e180ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[5][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
